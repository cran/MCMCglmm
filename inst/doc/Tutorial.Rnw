\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{Sweave}
% \SweaveOpts{width=60}
% \VignetteIndexEntry{MCMCglmm Tutorial}

\title{\bf{\Large{MCMCglmm: Markov chain Monte Carlo methods for Generalised Linear Mixed Models.}}}

\author{\bf{\large{J. D. Hadfield}}}


\begin{document}

\maketitle
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2cm, width=2cm]{Rlogo}
\end{center}
\end{figure}

\newpage
\tableofcontents
\newpage


MCMCglmm is a package for fitting Generalised Linear Mixed Models using Markov chain Monte Carlo techniques \citep{Hadfield.2009b}. Most commonly used distributions are supported together with some useful but less popular ones such as the zero-inflated Poisson and the ordinal or nominal multinomial. Missing values and left, right and interval censoring are accommodated for all traits. The package also supports multi-response models where the responses can follow different types of distribution.  The package allows various residual and random-effect variance structures to be specified including heterogeneous variances, unstructured covariance matrices and random regression (e.g. random slope models).  Three special types of variance structure that can be specified are those associated with pedigrees (animal models), phylogenies (the comparative method) and measurement error (meta-analysis).  The package makes heavy use of results in \citet{Sorensen.2002} and \citet{Davis.2006} which taken together result in what is hopefully a fast and efficient routine. Most small to medium sized problems should take seconds to a few minutes, but large problems (> 20,000 records) are possible.  My interest is in evolutionary biology so there are also several functions for applying tensor analysis \citep{Rice.2004} to real data and functions for visualising and comparing matrices.


\begin{Schunk}
\begin{Sinput}
> library("MCMCglmm")
\end{Sinput}
\end{Schunk}

\section{An Empirical Example}

To demonstrate how to fit and interpret these models we will use some data from a pilot study on the Indian meal moth (\emph{Plodia interpunctella}) and its granulosis virus, PiGV.  This data was collected by Hannah Tidbury \& Mike Boots at the University of Sheffield, and highlights a range of model fitting problems that are hard to solve with standard techniques. The aim of the study was to see if there was an association between the amount of phenoloxidase (PO) produced by the caterpillars and resistance to being artificially infected by the virus (measured as successfully reaching pupation). The difficulty with the experiment is that measuring PO is lethal, and so it is not possible to have individual measures for both PO and resistance to the virus. The solution was to take full-sib families and measure half the individuals for PO and infect the other half with the virus. The aim then was to estimate the `genetic' correlation between the two measures.  

\begin{Schunk}
\begin{Sinput}
> data(PlodiaPO)
\end{Sinput}
\end{Schunk}

We'll start with a simple analysis of PO which was box-cox transformed so that it was approximately normal.

\begin{Schunk}
\begin{Sinput}
> model1 <- MCMCglmm(PO ~ 1, random = ~FSfamily, data = PlodiaPO, 
+     verbose = FALSE)
\end{Sinput}
\end{Schunk}

We've modelled an in intercept \texttt{$\sim$1} and family effects \texttt{$\sim$FSfamily}.  \texttt{MCMCglmm} returns the output as \texttt{mcmc} objects from the coda package. The first of these is \texttt{Sol} which returns the fixed effect estimates:


\begin{Schunk}
\begin{Sinput}
> plot(model1$Sol)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-006}
\end{center}
\caption{Posterior distribution of the intercept from model1}
\label{model1Sol-fig}
\end{figure}


On the left of Figure \ref{model1Sol-fig} is a time series of the parameter as the MCMC iterates, and on the right is a posterior density estimate of the parameter (a smoothed histogram of the output).  Making inferences about the parameter(s) from the MCMC output is simple. For example, the most likely value is where the posterior distribution peaks:

\begin{Schunk}
\begin{Sinput}
> posterior.mode(model1$Sol)
\end{Sinput}
\begin{Soutput}
(Intercept) 
   1.163132 
\end{Soutput}
\end{Schunk}

The probability that the intercept is greater than 1.15 is simply the proportion of the output that is greater than 1.15:

\begin{Schunk}
\begin{Sinput}
> table(model1$Sol > 1.15)/1000
\end{Sinput}
\begin{Soutput}
FALSE  TRUE 
0.202 0.798 
\end{Soutput}
\end{Schunk}

and the region of 95\% support can be obtained by finding the interval with the highest posterior density:

\begin{Schunk}
\begin{Sinput}
> HPDinterval(model1$Sol, 0.95)
\end{Sinput}
\begin{Soutput}
               lower    upper
(Intercept) 1.135162 1.197875
attr(,"Probability")
[1] 0.95
\end{Soutput}
\end{Schunk}

However, these metrics are only an accurate description of the true posterior distribution when the chain has converged (there is no systematic trend in the time series) and when each successive value in the output does not show strong dependence with the previous one. The level of dependence can be measured using an autocorrelation statistic:

\begin{Schunk}
\begin{Sinput}
> autocorr(model1$Sol)
\end{Sinput}
\begin{Soutput}
, , (Intercept)

        (Intercept)
Lag 0    1.00000000
Lag 10  -0.05060955
Lag 50  -0.01527613
Lag 100 -0.05677407
Lag 500  0.03530793
\end{Soutput}
\end{Schunk}

As you can see, the autocorrelation between successive values (Lag 1) is small and we probably have obtained close to 1000 independent samples from the posterior. The chain is said to have mixed well. If the chain had not converged or mixed we could extend the burn-in in period (\texttt{burnin}), increase the total number of iterations (\texttt{nitt}) or increase the interval between which successive samples are stored (\texttt{thin}).\\

By default \texttt{MCMCglmm} will not store the estimates for each family effect, as generally we're not so interested in individual families but about the variation in these family effects across families.  Variance estimates are stored in the object \texttt{VCV}:

\begin{Schunk}
\begin{Sinput}
> plot(model1$VCV)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-012}
\end{center}
\caption{Posterior distribution of the variance components from  \texttt{model1}}
\label{model1VCV-fig}
\end{figure}


There are two variances, one associated with family effects and one associated with \texttt{units} (Fig \ref{model1VCV-fig}). \texttt{units} is a reserved variable which has a factor level for each row of the response. The default in \texttt{MCMCglmm} is to specify the residual term as \texttt{rcov=$\sim$units}. In this instance each data point corresponds to a unique level of \texttt{units} and therefore we simply interpret the units variance as we would the residual variance in most models. Often we're interested in knowing the proportion of total variance explained by the random effect, which would involve dividing the family variation by the sum of the two variance components. Obtaining some measure of confidence for this proportion is not straightforward using standard methods such as REML, but with MCMC it is easy. For example, the region of 95\% support:

\begin{Schunk}
\begin{Sinput}
> HPDinterval(model1$VCV[, "FSfamily"]/(model1$VCV[, "FSfamily"] + 
+     model1$VCV[, "units"]))
\end{Sinput}
\begin{Soutput}
         lower     upper
var1 0.1295410 0.3242785
attr(,"Probability")
[1] 0.95
\end{Soutput}
\end{Schunk}

A valid posterior for any transformation of model parameters can be obtained by applying that transformation to each sample and analysing the result.\\

This model was fitted without explicitly specifying a prior and the default priors were used. In certain circumstances this can lead to inferential and numerical problems\footnote{When this results in numerical problems (variances close to zero or correlations close to -1 or 1) MCMCglmm can fail, although I've tried make it do so nicely.} and Section \ref{prior-subsec} should be read carefully. To assess the sensitivity of the analysis to alternative prior specifications lets have a weak prior (nu=1) equal to half the variance in PO for each variance component:

\begin{Schunk}
\begin{Sinput}
> halfV <- var(PlodiaPO$PO)/2
> prior = list(R = list(nu = 1, V = halfV), G = list(G1 = list(nu = 1, 
+     V = halfV)))
> model2 <- MCMCglmm(PO ~ 1, random = ~FSfamily, data = PlodiaPO, 
+     verbose = FALSE, prior = prior)
> plot(mcmc.list(model1$VCV, model2$VCV))
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-015}
\end{center}
\caption{MCMC output from  \texttt{model1} (black trace) which was fitted without a prior on the variance components (nu=0) and \texttt{model2} in which a weak but proper prior (nu=1, V=0.02) was used (red trace) }
\label{model2VCV-fig}
\end{figure}

As you can see from Figure \ref{model2VCV-fig} the two models do not completely coincide but they are very close; most of the information is coming from the data.

\subsection{Simple univariate model - Binomial response}

The data on resistance to the virus have been aggregated into a binomial response for each family

\begin{Schunk}
\begin{Sinput}
> data(PlodiaR)
\end{Sinput}
\end{Schunk}

For multinomial data, of which the binomial is a special case, we need to specify for each category (in this case \texttt{Pupated} (not infected) and \texttt{Infected}) the number of counts  that were recorded, and we do this by passing the  multiple response variables using \texttt{cbind}

\begin{Schunk}
\begin{Sinput}
> model3 <- MCMCglmm(cbind(Pupated, Infected) ~ 1, family = "multinomial2", 
+     data = PlodiaR, verbose = FALSE)
\end{Sinput}
\end{Schunk}

Note that the family is \texttt{multinomial} and the final number is the number of categories (in this case 2). Also, we have not fitted a random effect. Because the data are counts for each family we cannot fit \texttt{FSfamily} levels as random effects because they would be confounded with the residuals.  The residual  variance is however estimated and  we can interpret this over-dispersion as variation in the binomial probability across families.  Figure \ref{model3VCV-fig} shows that there is ample over-dispersion (the variance is estimated to be well away from zero) leading us to believe that families vary in their probability of resistance.  


\begin{Schunk}
\begin{Sinput}
> plot(model3$VCV)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-019}
\end{center}
\caption{Residual variance from  \texttt{model3} which models binomial over-dispersion}
\label{model3VCV-fig}
\end{figure}

Variation in the probabilities are modelled on the logit scale and are assumed to be normal on that scale. We can get a feel for what this looks like on the data scale by getting the posterior modes for the intercept and the variance, and generating random numbers from this distribution. It is clear from Figure \ref{PMprob-fig} that the variation between families in the probability of resistance is non-negligible: many families are likely to have a 0.1 probability but many are also likely to have a 0.5 probability.

\begin{Schunk}
\begin{Sinput}
> PMmu <- posterior.mode(model3$Sol)
> PMv <- posterior.mode(model3$VCV)
> PMprob <- inv.logit(rnorm(10000, PMmu, sqrt(PMv)))
> hist(PMprob)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-021}
\end{center}
\caption{The predicted distribution of resistance probabilities across families assuming the posterior modes of \texttt{model3} are correct.}
\label{PMprob-fig}
\end{figure}

\newpage

\subsection{Simple univariate model - Binary response}

There is another way we could have fitted \texttt{model3}, and that is by expanding the binomial response for each family into a binary response for each individual within each family.

\begin{Schunk}
\begin{Sinput}
> data(PlodiaRB)
> prior = list(R = list(V = 1, nu = 0, fix = 1), G = list(G1 = list(V = 1, 
+     nu = 0)))
> model4 <- MCMCglmm(Pupated ~ 1, random = ~FSfamily, family = "categorical", 
+     data = PlodiaRB, prior = prior, verbose = FALSE)
\end{Sinput}
\end{Schunk}

Binary responses (or other responses that are discrete) are of the family \texttt{"categorical"}.  They pose a special problem because the residual variance cannot be estimated because the variance is uniquely determined by the mean. When proper priors are used this does not pose a problem to a Bayesian analyst, but we do have to be aware that all the information regarding the parameter is coming from the prior.  Because of this it is usual to fix these parameters at some value (I use 1 for variances and 0 for covariances) and this can be done by specifying \texttt{fix=1} in the appropriate prior element.  An identical procedure for simple models such as this would be to specify a very informative prior (\texttt{n} is large) around \texttt{V}:

\begin{Schunk}
\begin{Sinput}
> prior = list(R = list(V = 1, nu = 1e+06), G = list(G1 = list(V = 1, 
+     nu = 0)))
\end{Sinput}
\end{Schunk}

You will notice that the parameters of \texttt{model3} and \texttt{model4} do not take on the same values.  This is because in \texttt{model3} we implicitly assumed that the within individual variance was zero (as in standard quasi-binomial models) rather than 1.  This is essentially an arbitrary choice because we have no way of seeing this variation in real data.  This may seem a bit disconcerting but it should not worry you unduly; the models are just reparameterisations of each other and we can see this by approximating the posterior distribution of the mean on the data scale: 

\begin{Schunk}
\begin{Sinput}
> mu.data.scale <- function(logit.mu, logit.var) {
+     mean(inv.logit(rnorm(10000, logit.mu, sqrt(logit.var))))
+ }
> mu.model3 <- mcmc(mapply(mu.data.scale, model3$Sol, model3$VCV))
> mu.model4 <- mcmc(mapply(mu.data.scale, model4$Sol, rowSums(model4$VCV)))
> summary(mu.model3)
\end{Sinput}
\begin{Soutput}
Iterations = 1:1000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
     0.2956265      0.0246688      0.0007801      0.0006351 

2. Quantiles for each variable:

  2.5%    25%    50%    75%  97.5% 
0.2491 0.2788 0.2949 0.3120 0.3444 
\end{Soutput}
\begin{Sinput}
> summary(mu.model4)
\end{Sinput}
\begin{Soutput}
Iterations = 1:1000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 1000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
     0.2939079      0.0265516      0.0008396      0.0012199 

2. Quantiles for each variable:

  2.5%    25%    50%    75%  97.5% 
0.2446 0.2759 0.2929 0.3106 0.3479 
\end{Soutput}
\end{Schunk}

The posterior summaries are virtually identical up to Monte Carlo error.

\subsection{Bivariate model - Binary and Gaussian response}

Expanding the binomial data into binary data allows us to fit a model in which we can estimate what we're interested in: the correlation between PO and probability of resistance at the family level.  We start by creating missing PO values for those individuals in the resistance experiment and missing Pupated/Infected values for those individuals only measured for PO. We can do this by creating id's for each data point and then merging the two data frames:


\begin{Schunk}
\begin{Sinput}
> PlodiaPO$ID <- 1:dim(PlodiaPO)[1]
> PlodiaRB$ID <- dim(PlodiaPO)[1] + 1:dim(PlodiaRB)[1]
> PlodiaPORB <- merge(PlodiaPO, PlodiaRB, all = TRUE)
\end{Sinput}
\end{Schunk}

Fitting a sensible model to bivariate data requires a bit more work, because usually we will need to specify different models for each response but also specify a model that accounts for dependence between the two responses (otherwise we may as well just fit to univariate models). Fitting an appropriate model to the Plodia data is particularly fiddly but it serves as a useful example of the types of variance structures that can be fitted. 
 
\begin{Schunk}
\begin{Sinput}
> prior = list(R = list(V = diag(2), nu = 0, fix = 2), G = list(G1 = list(V = diag(2), 
+     nu = 1)))
> model5 <- MCMCglmm(cbind(PO, Pupated) ~ trait - 1, random = ~us(trait):FSfamily, 
+     rcov = ~idh(trait):units, family = c("gaussian", "categorical"), 
+     data = PlodiaPORB, prior = prior, verbose = FALSE)
\end{Sinput}
\end{Schunk}

For multivariate models we will usually want to make use of the reserved variable  \texttt{trait} which indexes columns of the response. By fitting \texttt{trait} as a fixed effect we allow the two responses to have different means. I usually fit the model \texttt{$\sim$trait-1} rather than just \texttt{$\sim$trait} as this fits trait specific intercepts rather than an intercept for trait 1 (\texttt{PO}) and a contrast for trait 2 (\texttt{Pupated}).  Likewise, just fitting a single family variance using  \texttt{$\sim$FSfamily} implies that families have respond in the same way to both traits, so we usually want to form an interaction.  The simplest would be \texttt{$\sim$trait:FSfamily}  but this would imply that the family variance for each trait is equal and the family effects for trait 1 are uncorrelated with the family effects for trait 2.  More reasonable variance structures can be formed using \texttt{idh} and \texttt{us}. \texttt{idh} allows different variances across the traits but assumes that the family effects for trait 1 are uncorrelated with the family effects for trait 2.   \texttt{us} is the most general variance structure and allows different variances across the traits and allows covariances to exist between them. In both these cases the variance structure becomes a 2x2 matrix (because there are two traits) rather than a scalar (1x1 matrix).

\begin{equation}
\begin{array}{c}
\texttt{idh(trait):FSfamily}  = 
\left[
\begin{array}{cc}
\sigma^{2}_{F(T_{1})}&0\\
0&\sigma^{2}_{F(T_{2})}\\
\end{array}
\right]\\
\texttt{us(trait):FSfamily}  = 
\left[
\begin{array}{cc}
\sigma^{2}_{F(T_{1})}&\sigma_{F(T_{1}, T_{2})}\\
\sigma_{F(T_{2}, T_{1})}&\sigma^{2}_{F(T_{2})}\\
\end{array}
\right]\\
\end{array}
\end{equation}

Here, $\sigma^{2}_{F(T_{1})}$ stands for the variance across families ($F$) for trait 1 ($T_{1}$) and $\sigma_{F(T_{1}, T_{2})}$ the covariance between trait 1 and trait 2 across families. Section \ref{var-subsec} covers this in more detail.\\

In \texttt{model5} we have used an \texttt{idh} structure for the residual component because both traits have never been measured on the same individual so the residual covariance cannot be estimated.  As before we have fixed the residual variance of the binary trait to 1 because it too cannot be estimated from the data.  Family members on the other hand have been measured for both traits and the covariance can be estimated, indeed it was the purpose of the experiment.\\

We can get the posterior distribution of the correlation between family effects by using the distribution of the correlation estimated for each iteration of the chain (Figure \ref{Pcor-fig})
   
\begin{Schunk}
\begin{Sinput}
> Pcor <- model5$VCV[, 2]/sqrt(model5$VCV[, 1] * model5$VCV[, 4])
> plot(Pcor)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-028}
\end{center}
\caption{The posterior distribution of the family level correlation between resistance and  PO from \texttt{model5}.}
\label{Pcor-fig}
\end{figure}

\subsection{An Animal model}

Quantitative geneticists will have noticed that the heritability of PO is not the proportion of variance explained by family  in \texttt{model1}, because full-sibs only share 50\%  of their genes.  With such a simple experimental design the estimate for the heritability can be obtained easily by multiplying the proportion of variance explained by family by two (Figure \ref{model1H2-fig}):


\begin{Schunk}
\begin{Sinput}
> h2 <- 2 * model1$VCV[, "FSfamily"]/(model1$VCV[, "FSfamily"] + 
+     model1$VCV[, "units"])
> plot(h2)
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-030}
\end{center}
\caption{The posterior distribution of PO heritability from \texttt{model1}.}
\label{model1H2-fig}
\end{figure}

In this instance the proportion of the variance explained by family was small enough and known with enough precision that the posterior did not exceed 0.5.  If it had of done then the resulting heritability would have had support at values greater than 1. This would be inconsistent with the  model because it would mean that genes explain more of the variation than actually exists.  An alternative model is to explicitly factor in the proportion of genes shared by two individuals. This has the advantage that the genetic variance can't exceed the total variance, and allows more complicated family structures to be modelled.  For example, when some individuals share 50\% of their genes and others 25\% and so on.  This type of model is known as an animal model and to be able to fit them we need to have a pedigree table that represents the genealogy of the individuals.  This is a 3 column table with id, mother, and father in each column.  Individuals with unknown parents have \texttt{NA} for their mother and/or father, and all individuals that appear as parents must be represented in the id column. The Plodia data were collected on 50 families named \texttt{F1} through to \texttt{F50}. We can make a pedigree for the individuals in the data by adding `dummy' parents (e.g. \texttt{F1mother} and  \texttt{F1father}):  


\begin{Schunk}
\begin{Sinput}
> ID <- c(paste("F", 1:50, "mother", sep = ""), paste("F", 1:50, 
+     "father", sep = ""), PlodiaPO$ID)
> DAM <- c(rep(NA, 100), paste(PlodiaPO$FSfamily, "mother", sep = ""))
> SIRE <- c(rep(NA, 100), paste(PlodiaPO$FSfamily, "father", sep = ""))
> pedigree <- cbind(ID, DAM, SIRE)
\end{Sinput}
\end{Schunk}


The model has the same form as \texttt{model1} except \texttt{animal} is fitted as a random effect rather than \texttt{FSfamily}. \texttt{animal} is a special variable in \texttt{MCMCglmm} and it will always be associated with the id levels in the first column of the pedigree.

   
\begin{Schunk}
\begin{Sinput}
> PlodiaPO$animal <- 1:dim(PlodiaPO)[1]
> model6 <- MCMCglmm(PO ~ 1, random = ~animal, data = PlodiaPO, 
+     pedigree = pedigree, verbose = FALSE)
\end{Sinput}
\end{Schunk}

A warning message appears saying that missing records have been added. This is because there are 100 parents in the pedigree file that do not have data records.  To allow the model to run these missing data are augmented and integrated over. For this experiment, fitting the model as an  animal model is inefficient because of this and the chain mixes poorly compared to \texttt{model1} 

\begin{Schunk}
\begin{Sinput}
> autocorr(model6$VCV)
\end{Sinput}
\begin{Soutput}
, , animal

             animal       units
Lag 0    1.00000000 -0.86763896
Lag 10   0.78517294 -0.74248076
Lag 50   0.37191247 -0.33150660
Lag 100  0.10782276 -0.08179846
Lag 500 -0.01659453  0.02127845

, , units

             animal       units
Lag 0   -0.86763896  1.00000000
Lag 10  -0.74358025  0.69515408
Lag 50  -0.33737790  0.30138629
Lag 100 -0.09836561  0.06664480
Lag 500  0.03351761 -0.03014689
\end{Soutput}
\end{Schunk}

That being said the two analyses give the same answer we just have to run the second for longer to get the same degree of accuracy\footnote{note that they are not perfectly identical because the default prior of \texttt{nu=0} is not uninformative. For a single variance component \texttt{V=0, nu=-2} is uninformative and using these priors the two methods give indistinguishable results at moderate heritabilities.}(Figure \ref{model6H2-fig}):

\begin{Schunk}
\begin{Sinput}
> h2AM <- model6$VCV[, 1]/(model6$VCV[, 1] + model6$VCV[, 2])
> plot(mcmc.list(h2, h2AM))
\end{Sinput}
\end{Schunk}

\begin{figure}[!h]
\begin{center}
\includegraphics{Tutorial-035}
\end{center}
\caption{The posterior distribution of PO heritability from the animal model \texttt{model6}.}
\label{model6H2-fig}
\end{figure}

 
\section{A Mathematical Tour}

The empirical examples only cover a small set of possible models that are possible using \texttt{MCMCglmm}. In this section I cover a wider range of models using a combination of code and maths.  I will use a notation that is flexible enough to cover multivariate generalised linear mixed models when the multiple responses come from different distributions. The multiple responses are passed to \texttt{MCMCglmm} as a matrix (${\bf Y}$) using \texttt{cbind()} but it will be easier to think of them concatenated column-wise into a vector denoted by ${\bf y}$.  Each element of ${\bf y}$ is associated with some probability distribution from the exponential family such as the Poisson or the normal (See Table \ref{Dist-tab}). I denote the canonical parameter of the distribution as $l$ as it is often called a latent variable or liability in quantitative genetics.  The canonical parameter is related to the more familiar distribution parameter through the canonical link function $g$(). For example, the mean of the Poisson distribution $\lambda = g^{-1}(l)$ where $g$() is log() and $g^{-1}$() is exp().  The probability of the $i^{th}$ data point conditional on $l_{i}$ is denoted as

\begin{equation}
p_{i}(y_{i} | l_{i})
\end{equation}

where $p_{i}$ is the probability density function associated with $y_{i}$. The linear mixed model is applied to $l$ not $y$:

\begin{equation}
{\bf l}  = {\bf X}{\bm \beta}+{\bf Z}{\bf u}+{\bf e}
\label{EqModel}
\end{equation}

A special case is when $p_{i}$ is the normal distribution. In this case $l_{i} = y_{i}$ (because the canonical link function is the identity link) and the probability $p_{i}(y_{i} | l_{i} = y_{i})$ is always unity. In this case we can replace the LHS of Equation \ref{EqModel} with ${\bf y}$ to obtain the standard linear mixed model. The notation for the linear model is fairly standard:  ${\bf X}$ is a design matrix relating fixed predictors to the data and ${\bf Z}$ is a design matrix relating random predictors to the data.  These predictors have associated parameter vectors ${\bm \beta}$ and ${\bf u}$. In Bayesian analyses both these parameter vectors are technically random, but I will stick with the frequentist terminology of referring to them as fixed effects and random effects, respectively.  The key difference between ${\bm \beta}$ and ${\bf u}$ is that $u$'s are assumed to come from some distribution, the parameters of which are usually estimated, whereas the $\beta$'s are not\footnote{This is not strictly true in a Bayesian analysis because ${\bm \beta}$ are assumed to be distributed according to the prior}. MCMCglmm assumes that the $n_{u}$ $u$'s follow a $n_{u}$-dimensional multivariate normal distribution with null mean vector and a structured (co)variance matrix. We will call this structure the G-structure.  ${\bf e}$ is a vector of residuals which also come from a structured multivariate normal distribution the parameters of which are usually estimated.  We will refer to this (co)variance structure as the R-structure.\\

The variables \texttt{trait} and \texttt{units} can be used as predictors in the model formulae to index the column and row to which an element of ${\bf l}$ originally belonged in ${\bf L}$, where ${\bf L}$ is the liability equivalent of the data before concatenation (${\bf Y}$). The distribution associated with a data point can be specified in one of two ways. The easiest way is to specify them in the \texttt{family} argument where each distribution name is associated with a column of ${\bf Y}$. Alternatively,  \texttt{family=NULL} can be specified if \texttt{data} contains a variable \texttt{family}.\footnote{only \texttt{gaussian}, \texttt{poisson}, \texttt{exponential} and \texttt{categorical} distributed data can be specified this way, not variables where the dimension of ${\bf Y}$ and ${\bf L}$ are not equal. Note that this would be a much more efficient way of fitting \texttt{model5} in the example section.}\\

\subsection{Variance structures}
\label{var-subsec}

The standard variance structure is to have effects i.i.d (independent and identically distributed). In matrix form this is represented by a diagonal matrix with a single variance parameter. For example, 

 \begin{equation}
 {\bf R}  = 
\left[
 \begin{array}{cccc}
 \sigma^{2}_{e}&0&\dots&0\\
 0&\sigma^{2}_{e}&\dots&0\\
 \vdots&\vdots&\ddots&\vdots\\
0&0&\dots&\sigma^{2}_{e}\\ 
\end{array}
\right]  =
\sigma^{2}_{R}{\bf I}
\label{var1-eq}
\end{equation}

where all covariances are set to zero (implying independence) and all diagonal elements are the same (implying identically distributed). However, there are alternatives.  Imagine that two traits are both measured on a set of  individuals. It would be natural to have different residual variances for each trait, and also allow residual covariances between the two traits measured in the same individual.  The appropriate R structure would then have the form:

      
 \begin{equation}
 {\bf R}  = {\bf V}_{R}\otimes{\bf I} =
\left[
 \begin{array}{cccccc}
 \sigma^{2}_{e_{1}}&0&\dots&\sigma_{e_{1}, e_{2}}&0&\dots\\
 0& \sigma^{2}_{e_{1}}&\dots&0&\sigma_{e_{1}, e_{2}}&\dots\\
 \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\
\sigma_{e_{2}, e_{1}}&0&\dots&\sigma^{2}_{e_{2}}&0&\dots\\
0&\sigma_{e_{2}, e_{1}}&\dots&0&\sigma^{2}_{e_{2}}&\dots\\
\vdots&\vdots&\vdots&\vdots&\vdots&\ddots\\
\end{array}
\right]
\end{equation}

where the residual covariance matrix has the form

\begin{equation}
{\bf V}_{R} = \left[
 \begin{array}{cc}
 \sigma^{2}_{e_{1}}&\sigma_{e_{1}, e_{2}}\\
 \sigma_{e_{2}, e_{1}}&\sigma^{2}_{e_{2}}\\
\end{array}
\right] 
\end{equation}

This type of structure can be fitted using the \texttt{us} function: \texttt{us(trait):individual}.\\ 

If the two traits  were measured on different individuals then there would be no reason to expect covariation between the residuals and an appropriate residual covariance matrix may be:

\begin{equation}
{\bf V}_{R} =
\left[
 \begin{array}{cc}
 \sigma^{2}_{e_{1}}&0\\
0&\sigma^{2}_{e_{2}}\\
\end{array}
\right] 
\end{equation}

giving

 \begin{equation}
 {\bf R}  = {\bf V}_{R}\otimes{\bf I} = 
\left[
 \begin{array}{ccccc}
 \sigma^{2}_{e_{1}}&0&\dots&0&0\\
 0& \sigma^{2}_{e_{1}}&\dots&0&0\\
 \vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\dots&\sigma^{2}_{e_{2}}&0\\
0&0&\dots&0&\sigma^{2}_{e_{2}}\\
\end{array}
\right]  
\label{var5-eq}
\end{equation}

implying independence but not identical distribution.  This type of structure can be fitted using the \texttt{idh} function: \texttt{idh(trait):individual}.\\ 

Only a single variance structure can define the R-structure, but more than one random effect can be fitted by using the \texttt{+} operator, and the the resulting G-structure is formed from the direct sum of the variance structures.  This sounds complicated but it just means that more than one random effect can be fitted and that there is no covariance between them.\\ 


\subsection{Pedigrees and Phylogenies}

The variance structures above are formed through a Kronecker product involving an identity matrix, and therefore have a simple and highly patterned form. For certain types of model the identity matrix is not appropriate because covariances may be expected between different levels of the random effect. For example, if individuals are related we may expect them to be more similar if the response variable is heritable.  However, if individuals are related to different degrees (e.g. some brothers, some second cousins) we don't expect them to be equally similar.  Under certain assumptions we can predict the degree of similarity through the proportion of genes shared by two individuals. This can be represented using the matrix ${\bf A}$, and for a nuclear family consisting of two parents followed by their two children this would look like:

\begin{equation}
{\bf A}  = 
\left[
 \begin{array}{cccc}
1&0&0.5&0.5\\
0&1&0.5&0.5\\
0.5&0.5&1&0.5\\
0.5&0.5&0.5&1\\
\end{array}
\right]  
\end{equation}

The diagonal elements are one because an individual shares all of its genes with itself\footnote{I've left the interpretation a little loose; the diagonals are not necessarily one with inbreeding.} and the off-diagonal elements between parents and offspring, and between siblings, are 0.5 because on average they share 50\% of their genes. The diagonal element between the parents is zero because they are assumed to be unrelated.  This model is known as an animal model \citep{Henderson.1976}, and when \texttt{animal} is fitted as a random effect then ${\bf I}$ in Equations \ref{var1-eq}:\ref{var5-eq} is replaced with ${\bf A}$, where ${\bf A}$ is calculated from a pedigree passed to the \texttt{pedigree} argument of \texttt{MCMCglmm}.  For example, the syntax \texttt{us(trait):animal} in a bivariate model would fit a G-structure of the form  ${\bf V}_{G}\otimes{\bf A}$ where ${\bf V}_{G}$ has the genetic variances for trait 1 and trait 2 along the diagonal, and the genetic covariance between the two traits in the off-diagonals.  The same model can be applied to phylogenies where a \texttt{phylo} object from the \texttt{ape} package is passed to the pedigree argument. In this case the \texttt{animal} term would not contain individual identifiers but taxa names, possibly species.\\

\subsection{Meta-analysis}

In meta-analysis each data point has some associated measurement error which may be known. If the data are published test-statistics the variance around the true value due to measurement error may be assumed to be roughly the square of the standard error. These measurement error variances can be passed to \texttt{MCMCglmm} through the argument \texttt{mev} to fit a random effect meta-analysis.  The variance structure in this case will have the form  ${\bf V}_{G}\otimes{\bf D}$ where ${\bf D}$ is a diagonal matrix of measurement error variances and ${\bf V}_{G}$ is a scalar fixed at one.

 
\subsection{Random regression}

Another variation on the simple mixed effect model is random regression.  In standard models ${\bf Z}$ is a matrix of zeros and ones, with a one in row $i$ and column $j$ indicating that the $i^{th}$ data point is associated with the $j^{th}$ level of the random effect. However for random regression models, of which the random slope model is a special case, the elements of  ${\bf Z}$ can take on alternative numeric values. For example, lets imagine individuals were weighed twice, once at 6 months (time=0.5) and once after 18 months (time=1.5). If the individuals were grouped into families we could ask a) whether individuals of certain families are generally larger at both ages and b) whether individuals of certain families grow faster between the two time periods. For two individuals from different families, ${\bf Z}$ would have the form:


\begin{equation}
\left[
\begin{array}{ccccc}
1&0&\dots&0.5&0\\
1&0&\dots&1.5&0\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&1&\dots&0&0.5\\
0&1&\dots&0&1.5\\
\end{array}
\right]  
\end{equation}


where the first and second columns are associated with the (random) intercept's for families 1 and 2, and the third and fourth columns are associated with the (random) slopes for families 1 and 2. \texttt{us(1+time):FSfamily} fits a random intercept-slope model across families for the covariate time. The \texttt{us()} structure fits the covariance between intercept and slope but this could be set to zero using either \texttt{idh(1+time):FSfamily} or alternatively \texttt{idh(time):FSfamily+FSfamily}. Higher order polynomials can be fitted using either using the \texttt{poly} or \texttt{leg} functions; \texttt{us(1+time):FSfamily}, \texttt{us(leg(time,1,normalized=FALSE)):FSfamily} and \texttt{us(1+poly(time,1,raw=TRUE)):FSfamily} are all equivalent.

\subsection{Priors}
\label{prior-subsec}

The prior specification is passed to \texttt{MCMCglmm} by the \texttt{prior} argument. It takes a list of 3 elements: \texttt{R}, \texttt{G} and \texttt{B}, which specify the priors for the R-structure, G-structure and the fixed effects.  \texttt{G} is also a list with an element for each random effect. The covariance matrices are assumed to be (conditional)  inverse-Wishart distributed and individual elements for each variance structure take the arguments \texttt{V}, \texttt{n} and \texttt{fix} which specify the (co)variance matrix, the degree of freedom parameter, and the partition to condition on.  For example, the residual error structure for a trivariate model may look something like:

\begin{Schunk}
\begin{Sinput}
> prior = list(R = list(V = diag(3), nu = 3))
\end{Sinput}
\end{Schunk}

which is inverse Wishart distributed with inverse scale matrix \texttt{solve(n*V)} and degree of belief parameter \texttt{n}.  You can get a feel for this using the \texttt{plotprior} function which generates random samples from the distribution and plots the resulting histogram. The default prior is \texttt{nu=0} and is not proper, but it is informative. The specification \texttt{V=diag(k)*0, nu=-(k+1)} where \texttt{k} is the dimension of \texttt{V} is uninformative, but I strongly recommend you use proper priors where \texttt{n} is greater than $k-1$ (or more if some (co)variances cannot be identified from the likelihood). If nothing else, \texttt{MCMCglmm} may baulk after many iterations if the parameters are not well identified. When \texttt{idh()} structures are used each diagonal element of the matrix is independently distributed \emph{a priori}. The distribution of the diagonal element has the same marginal distribution as that element would have in the standard inverse-Wishart distribution (i.e. $\sim IW(n-k+1, (n\sigma^{2})^{-1})$) such that the prior specification above would actually be equivalent to three priors on each diagonal element with \texttt{V=1} and \texttt{nu=1}.  It is also worth noting that in the univariate case the prior specification \texttt{V=1} and \texttt{n=0.002} is equivalent to an inverse gamma distribution with $\alpha=\beta=0.001$, which is often used in WinBUGS.\\  

The fixed effects have a multivariate normal distribution equal in dimension to the number of fixed effects. \texttt{B} has two arguments a mean vector \texttt{mu} and a (co)variance matrix \texttt{V}. The default has a zero mean vector and a diagonal variance matrix with large variances (1e+10). 

\subsection{Fixing (co)variances}

In meta-analysis we fix the variance component to one because we are assuming we know the measurement error variances. In some instances however, we may want to fix the variances because there is no information in the data and we cannot estimate them. We can fix certain elements of a variance structure by giving \texttt{fix} a value in the prior specification.  For example, we could modify the previous prior specification:

\begin{Schunk}
\begin{Sinput}
> prior = list(R = list(V = diag(3), nu = 3, fix = 2))
\end{Sinput}
\end{Schunk}
 
The fix argument partitions V into (potentially) 4 sub-matrices where the partition occurs on the \texttt{fix}$^{th}$ diagonal element. In this case the partition has the form

\begin{equation}
 {\bf V} = \left[
\begin{array}{c|cc}
1&0&0\\
\hline
0&1&0\\
0&0&1\\
\end{array}
\right]
\end{equation}

and the lower right sub-matrix is fixed and not estimated. When \texttt{fix=1} the whole matrix is fixed. The default prior for the variance structures do not fix any component.\\

\subsection{Starting values}

Starting values are passed to \texttt{MCMCglmm} by the \texttt{start} argument. It takes a list of 4 elements: \texttt{R}, \texttt{G} and \texttt{liab} specify the starting R-structure, G-structures and the liabilities.  As with the prior argument, \texttt{G} must also be a list with an element for each random effect. The starting liabilities are passed as a matrix equal in dimension to ${\bf L}$.  By default \texttt{MCMCglmm} will use some very rough and ready methods for obtaining reasonable starting values for the liabilities, however, this can be suppressed by passing the fourth element of \texttt{start} as \texttt{QUASI=FALSE} which will then sample the starting liabilities from a normal distribution with mean zero and variance one when the \texttt{liab} element is \texttt{NULL}. 

\subsection{Tuning parameters}

For certain types and combinations of distribution the liabilities have to be sampled using Metropolis-Hastings (MH) updates rather than Gibbs sampling.  The choice of proposal distribution for the MH sampler can strongly effect the rate of convergence and the mixing properties of the chain.  By default an adaptive MH sampler is used which modifies the proposal distribution so it has an efficient rate of jumping. Once the burnin period is over this proposal distribution is fixed so as to ensure that the posterior distribution is valid. Alternatively, a matrix equal in dimension to the R-structure can be passed to the tune argument, and this will serve as the covariance matrix for the proposal distribution centered on the previous value of the liabilities.

\subsection{Distributions}

Currently the distributions listed in Table \ref{Dist-tab} are those that can be used, although this list will be extended to cover some additional distributions such as the Weibull.\\  

\begin{table}
\begin{center}
\small
\begin{tabular}{ccccc}
\hline
Distribution   &    No. Data       &         No. liability       &      Recommended &    Recommended\\
   Type        &  (${\bf Y}$) columns  &  (${\bf L}$) columns  &     R-structure &    R-constraint \\   
\hline\\
   \texttt{"gaussian"}        &  1  &   1  &    standard &    none  \\   
   \texttt{"poisson"}        &  1  &   1 &     standard &     none \\   
   \texttt{"categorical"}        &  1  &   $k$-1  &   \texttt{us(trait):units} &    \texttt{fix=1, V=}$\frac{1}{k}({\bf I}+{\bf J})$ \\   
   \texttt{"multinomial$k$"}  &  $k$    &  $k$-1  & standard  &    none \\   
   \texttt{"ordinal"}         &  1      &  1      & standard  &   \texttt{fix=1, V=1} \\   
   \texttt{"exponential"}         &  1  &   1  &     standard &    none \\   
\hline\\
   \texttt{"cengaussian"}        &  2 &   1  &     standard &    none \\   
   \texttt{"cenpoisson"}        &  2  &   1  &     standard &    none\\   
   \texttt{"cenexponential"}     &2    &  1  &   standard  &     none \\    
\hline\\
        &    &     &      &   \texttt{fix=2,}\\   
   \texttt{"zipoisson"}        &  1  &   2  &     \texttt{idh(trait):units} &   \texttt{V=as.matrix(c(a,0,0,1),2,2)}\\   
\hline\\
\end{tabular}
\caption{Distribution types that can fitted using \texttt{MCMCglmm}; their dimension and recommended residual variance structures.  For multi-trait analyses there are no hard and fast rules although \texttt{us(trait):units} is often a good starting point for the residual variance structure. The prefix \texttt{"cen"} standards for censored, and the prefix \texttt{"zi"} stands for zero-inflated. $k$ stands for the number of categories in the multinomial/categorical distributions and this must be specified in the family argument for the multinomial distribution. ${\bf I}$ is the identity matrix and ${\bf J}$ the unit matrix of all ones. In this context they both have dimension $k-1$}
\end{center}
\label{Dist-tab}
\end{table}

Multinomial and categorical models are parameterised in $k-1$ dimensions where $k$ is the number of categories. For multinomial data that only have a single count the data can be represented by a column of factors. If they are passed in this way then the baseline category is the first factor level. I've done this so that if a binary trait is passed, the model is predicting ones not zeros as is usual.  If the distribution is specified as multinomial$k$ then the data is expected to have $k$ columns. In this instance the final column is treated as the baseline category in keeping with the usual binomial specification \texttt{cbind(successes, failures)} where the model is predicting successes (not failures).\\    

For censored responses two data columns must be passed. The first column should contain the minimum value the data could take and the second column the maximum. If these values are finite the data are said to be interval censored. For left censored data use \texttt{-Inf} in the first column and for right censored data use \texttt{Inf} in the second column. If a particular data point is not censored have the same value in both columns.\\   

Although only a single column of data is passed for the zero-inflated Poisson, the response is actually expanded to form a bivariate model. The first \texttt{trait} is the Poisson part of the model and the residual variance can be estimated (to account for over-dispersion) but the second \texttt{trait} is the zero-inflation and as with a binary model the residual variance cannot be estimated. In addition the residual (co)variance between the Poisson term and the zero-inflation term cannot be estimated because we can't observe both processes in a single individual.  Nevertheless this does not mean that these covariances cannot be estimated at the level of some random effect.

\subsection{Deviance and DIC}

The deviance $D$ is defined as:

\begin{equation}
D = -2\texttt{log}(p({\bf y} | ...))
\end{equation}

where again $...$ stands for the model parameters.  This probability can be calculated in different ways depending on what is in `focus', and MCMCglmm calculates this probability for the lowest level of the hierarchy \citep{Spiegelhalter.2002}. For Gaussian response variables the likelihood is the density of ${\bf y}$ in the multinormal distribution:

\begin{equation}
N({\bf X}{\bm \beta}+{\bf Z}{\bf u}, {\bf R}) 
\end{equation}

but for other response variables variables it is simply the density $p({\bf y} | {\bf l})$. For multivariate models with mixtures of Gaussian and non-Gaussian data (including missing values) the likelihood of the Gaussian data is the density of ${\bf y}_{g}$ in the conditional multinormal distribution:

\begin{equation}
N({\bf X}_{g}{\bm \beta}+{\bf Z}_{g}{\bf u}+{\bf R}_{g,l}{\bf R}^{-1}_{l,l}({\bf l}-{\bf X}_{l}{\bm \beta}-{\bf Z}_{l}{\bf u}), {\bf R}_{g,g}-{\bf R}_{g,l}{\bf R}^{-1}_{l,l}{\bf R}_{l,g}) 
\end{equation}

where the subscripts $g$ and $l$ denote rows of the data vector/design matrices that pertain to Gaussian data, and non-Gaussian data respectively. Subscripts on the ${\bf R}$-structure index both rows and columns.\\

The deviance is calculated at each iteration if \texttt{DIC=TRUE} and stored each \texttt{thin}$^{th}$ iteration after burn-in.  The mean deviance ($\bar{D}$) is calculated over all iterations, as is the mean of the latent variables (${\bf l}$) the ${\bf R}$-structure and the vector of predictors (${\bf X}{\bm \beta}+{\bf Z}{\bf u}$).  The deviance is calculated at the mean estimate of the parameters ($D(\bar{...})$) and the deviance information criterion calculated as:

\begin{equation}
\texttt{DIC} = 2\bar{D}-D(\bar{...})
\end{equation}
   
Models with lower DIC are preferred.


\section{Acknowledgements}

This work would not have been possible without the CSparse library written by Tim Davis and the comprehensive book on MCMC and mixed models by Sorensen \& Gianola.   I am grateful to Loeske Kruuk for providing funding for this work through the Leverhulme Trust, and Hannah Tidbury and Mike Boots for allowing me to use their data to demonstrate the package. Laura Ross did battle with the tutorial, and serveral people have pointed out bugs with earlier versions; Gregor Gorjanc, Julien Martin, Michael Morrisey,  Shinichi Nakagawa, Matt Robinson, \& Sha Tao to name a few.  

\bibliographystyle{plainnat}

\bibliography{JarLib}


\end{document}
 

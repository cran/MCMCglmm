\newif\ifalone
\alonefalse
\ifalone
\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{Sweave}
\usepackage{lscape}
\usepackage{makeidx}

\title{Simultaneity \& Recursion}
\author{Jarrod Hadfield (\texttt{j.hadfield@ed.ac.uk})}

\begin{document}
\maketitle
\else
\chapter{Simultaneity \& Recursion}
\label{SandR}
\fi


There are many situations where it would seem reasonble to put some apsect of a response variable in as a predictor, and the only thing that stops us is some (often vague) notion that this is a bad thing to do from a statistical point of view. The approach appears to have a long history in economics but I came across the idea in a paper written by  \citet{Gianola.2004}. The notation of this section, and indeed the sampling strategy employed in MCMCglmm is derived from this paper.\\



\begin{equation}
{\bm \Lambda} = {\bf I} - \sum_{l}{\bf S}^{(l)}{\lambda_{l}}
\end{equation}

Having ${\bf X}_{\Lambda} = \left[{\bf S}^{(1)}\ {\bf S}^{(2)}\ \dots {\bf S}^{(L-1)}\ {\bf S}^{(L)}\right]$ we have 

\begin{equation}
\begin{array}{rl}
{\bm \Lambda} =& {\bf I}-{\bf X}_{\Lambda}\left({\bm \lambda} \otimes {\bf I} \right)\\
\end{array}
\label{rs-Eq}
\end{equation}

where ${\bm \lambda} = \left[\lambda_{1}\ \lambda_{2} \dots \lambda_{L-1}\ \lambda_{L} \right]^{\top}$, and:\\

\begin{equation}
\begin{array}{rl}
{\bf Y} =&  {\bf X}_{\Lambda}({\bf I}_{L} \otimes {\bf y})\\
\end{array}
\end{equation}


${\bf S}$ are square matrices of dimension $n \times n$. Element $s_{i,j}^{(l)}=k$ sets up the equation $y_{i} = \lambda_{l}ky_{j} \dots$.\\

Each ${\bf S}$ can be formed using the function \texttt{sir} which takes two formulae. ${\bf S} = {\bf X}_{1}{\bf X}_{2}^{\top}$ where ${\bf X}_{1}$ and ${\bf X}_{2}$ are the model matrices defined by the formulae (with intercept removed).  ${\bf X}_{1}$ and ${\bf X}_{2}^{\top}$ have to be conformable, and although this could be achieved in many ways, one way to ensure this is to have categorical predictors in each which have common factor levels.  To give a concrete example, lets take a sample of individuals measured a variable number of times for 2 traits: 

\begin{Schunk}
\begin{Sinput}
> id <- sample(1:100, 100, replace = T)
> y1 <- rnorm(100)
> y2 <- rnorm(100)
> y <- c(y1, y2)
> trait <- gl(2, 100)
\end{Sinput}
\end{Schunk}


Lets then imagine that each of these indiviuals interacts with another randomly chosen individual - indexed in the vector \texttt{id1}

\begin{Schunk}
\begin{Sinput}
> id1 <- sample(id, 100, replace = T)
> id <- as.factor(c(id, id))
> id1 <- factor(c(id1, id1), levels = levels(id))
\end{Sinput}
\end{Schunk}

we will adopt a recursive model where by the phenotypes of indiviuals in the \texttt{id1} vector affect those in the \texttt{id} vector:

\begin{Schunk}
\begin{Sinput}
> S <- sir(~id1, ~id)
\end{Sinput}
\end{Schunk}

we can see that the first record for individual \texttt{id[1]=}3 is directly affected by individual \texttt{id1[1]=}24's traits:

\begin{Schunk}
\begin{Sinput}
> S[1, which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
 10  58 110 158 
  1   1   1   1 
\end{Soutput}
\end{Schunk}

i.e indiviual  \texttt{id1[1]=}24 has 4 records.\\

We can build on this simple model by stating that only trait 2 affects trait 1:

\begin{Schunk}
\begin{Sinput}
> S <- sir(~id1:at.level(trait, 1), ~id:at.level(trait, 2))
> S[c(1, 101), which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
    10 58 110 158
1    0  0   1   1
101  0  0   0   0
\end{Soutput}
\end{Schunk}

or that trait 2 affect both trait 2 and trait 1:

\begin{Schunk}
\begin{Sinput}
> S <- sir(~id1, ~id:at.level(trait, 2))
> S[c(1, 101), which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
    10 58 110 158
1    0  0   1   1
101  0  0   1   1
\end{Soutput}
\end{Schunk}


\begin{Schunk}
\begin{Sinput}
> my.data <- data.frame(y1 = y1, y2 = y2, id = id[1:100], id1 = id1[1:100], 
+     x = rnorm(100))
> m1 <- MCMCglmm(y1 ~ x + sir(~id1, ~id) + y2, data = my.data, 
+     verbose = FALSE)
\end{Sinput}
\end{Schunk}

One problem is that ${\bf e}^{\star}$ the residual vector that appears in the likelihood for the latent variable does not have a simple (block) diagonal structure:

\begin{equation}
{\bf e}^{\star} \sim N\left({\bm 0}, {\bm \Lambda}^{-1}{\bf R}{\bm \Lambda}^{-\top}\right)
\end{equation}





\ifalone
\end{document}
\else
\fi


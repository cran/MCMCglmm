%Rnw("~/Work/AManal/UP_course/Tex/Lecture9/Lecture9.Rnw")
\newif\ifalone
\alonefalse
\ifalone
\documentclass{article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{Sweave}
\usepackage{lscape}
\usepackage{makeidx}



\title{Path Analysis \& Antedependence Structures}
\author{Jarrod Hadfield (\texttt{j.hadfield@ed.ac.uk})}

\begin{document}
\maketitle
\else
\chapter{Path Analysis \& Antedependence Structures}
\label{SandR}
\fi


There are many situations where it would seem reasonble to put some apsect of a response variable in as a predictor, and the only thing that stops us is some (often vague) notion that this is a bad thing to do from a statistical point of view. The approach appears to have a long history in economics but I came across the idea in a paper written by  \citet{Gianola.2004}. The notation of this section, and indeed the sampling strategy employed in MCMCglmm is derived from this paper.\\

\section{Path Anlaysis}

${\bf \Psi}^{(l)}$ is a square matrix of dimension $N \times N$ where $\Psi_{i,j}^{(l)}=k$ sets up the equation $y_{i} = \lambda_{l}ky_{j} \dots$. In matrix terms:

\begin{equation}
{\bm \Lambda}{\bf y} = {\bf y} - \sum_{l}{\bf \Psi}^{(l)}{\bf y}{\lambda_{l}}
\end{equation}

Having ${\bm \Psi} = \left[{\bm \Psi}^{(1)}\ {\bm \Psi}^{(2)}\ \dots {\bm \Psi}^{(L-1)}\ {\bm \Psi}^{(L)}\right]$ we have 

\begin{equation}
\begin{array}{rl}
{\bm \Lambda}{\bf y} =& {\bf y}-{\bm \Psi}\left({\bf I}_{L} \otimes {\bf y} \right){\bm \lambda}\\
                     =& {\bf y}-{\bm \Psi}\left({\bm \lambda}\otimes {\bf I}_{N}\right) {\bf y}\\
\end{array}
\label{rs-Eq}
\end{equation}

where ${\bm \lambda} = \left[\lambda_{1}\ \lambda_{2} \dots \lambda_{L-1}\ \lambda_{L} \right]^{\top}$, and ${\bm \Lambda}={\bf I}_{N}-{\bm \Psi}\left({\bm \lambda}\otimes {\bf I}_{N}\right)$\\

Each ${\bf \Psi}^{(l)}$ can be formed using the function \texttt{sir} which takes two formulae. ${\bf \Psi} = {\bf X}_{1}{\bf X}_{2}^{\top}$ where ${\bf X}_{1}$ and ${\bf X}_{2}$ are the model matrices defined by the formulae (with intercept removed).  ${\bf X}_{1}$ and ${\bf X}_{2}^{\top}$ have to be conformable, and although this could be achieved in many ways, one way to ensure this is to have categorical predictors in each which have common factor levels.  To give a concrete example, lets take a sample of individuals measured a variable number of times for 2 traits: 

\begin{Schunk}
\begin{Sinput}
> id <- sample(1:100, 100, replace = T)
> y1 <- rnorm(100)
> y2 <- rnorm(100)
> y <- c(y1, y2)
> trait <- gl(2, 100)
\end{Sinput}
\end{Schunk}


Lets then imagine that each of these indiviuals interacts with another randomly chosen individual - indexed in the vector \texttt{id1}

\begin{Schunk}
\begin{Sinput}
> id1 <- sample(id, 100, replace = T)
> id <- as.factor(c(id, id))
> id1 <- factor(c(id1, id1), levels = levels(id))
\end{Sinput}
\end{Schunk}

we will adopt a recursive model where by the phenotypes of indiviuals in the \texttt{id1} vector affect those in the \texttt{id} vector:

\begin{Schunk}
\begin{Sinput}
> Psi <- sir(~id1, ~id)
\end{Sinput}
\end{Schunk}

we can see that the first record for individual \texttt{id[1]=}10 is directly affected by individual \texttt{id1[1]=}48's traits:

\begin{Schunk}
\begin{Sinput}
> Psi[1, which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
 31  90 131 190 
  1   1   1   1 
\end{Soutput}
\end{Schunk}

i.e indiviual  \texttt{id1[1]=}48 has 4 records.\\

We can build on this simple model by stating that only trait 2 affects trait 1:

\begin{Schunk}
\begin{Sinput}
> Psi <- sir(~id1:at.level(trait, 1), ~id:at.level(trait, 
+     2))
> Psi[c(1, 101), which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
    31 90 131 190
1    0  0   1   1
101  0  0   0   0
\end{Soutput}
\end{Schunk}

or that trait 2 affect both trait 2 and trait 1:

\begin{Schunk}
\begin{Sinput}
> Psi <- sir(~id1, ~id:at.level(trait, 2))
> Psi[c(1, 101), which(id == id1[1])]
\end{Sinput}
\begin{Soutput}
    31 90 131 190
1    0  0   1   1
101  0  0   1   1
\end{Soutput}
\end{Schunk}


\begin{Schunk}
\begin{Sinput}
> my.data <- data.frame(y1 = y1, y2 = y2, id = id[1:100], 
+     id1 = id1[1:100], x = rnorm(100))
> m1 <- MCMCglmm(y1 ~ x + sir(~id1, ~id) + y2, data = my.data, 
+     verbose = FALSE)
\end{Sinput}
\end{Schunk}

One problem is that ${\bf e}^{\star}$ the residual vector that appears in the likelihood for the latent variable does not have a simple (block) diagonal structure when (as in the case above) the elements of the response vector that are regressed on each other are not grouped in the R-structure:

\begin{equation}
{\bf e}^{\star} \sim N\left({\bm 0}, {\bm \Lambda}^{-1}{\bf R}{\bm \Lambda}^{-\top}\right)
\end{equation}

Consequently, analyses that involve latent variables (i.e. non-Gaussian data, or analyses that have incomplete records for determining the R-structure) are currently not implemented in MCMCglmm.

The \texttt{path} function is a way of specifying path models that are less general than those formed by \texttt{sir} but are simple enough to allow updating of the latent variables associated with non-Gaussian data. Imagine a residual structure is fitted where the $N$ observations are grouped into $n$ blocks of $k$. For instance this might be $k$ different characteristics measured in $n$ individuals. A path model may be entertained whereby an individual's characteristics only affect their own characteristics rather than anyone elses. In this case, ${\bm \Psi}^{(l)}={\bm \psi}^{(l)}\otimes{\bf I}_{n}$ is block diagonal, and ${\bm \Psi}={\bm \psi}\otimes{\bf I}_{n}$ where ${\bm \psi} = \left[{\bm \psi}^{(1)}, {\bm \psi}^{(2)} \dots {\bm \psi}^{(L)}\right]$. Consequently,

\begin{equation}
\begin{array}{rl}
{\bm \Lambda}=&{\bf I}_{N}-{\bm \Psi}\left({\bm \lambda}\otimes {\bf I}_{N}\right)\\
=&{\bf I}_{N}-\left({\bm \psi}\otimes{\bf I}_{n}\right)\left({\bm \lambda}\otimes {\bf I}_{N}\right)\\
=&\left({\bf I}_{k}\otimes{\bf I}_{n}\right)-\left({\bm \psi}\otimes{\bf I}_{n}\right)\left({\bm \lambda}\otimes {\bf I}_{k}\otimes{\bf I}_{n}\right)\\
=&\left({\bf I}_{k}-{\bm \psi}\left({\bm \lambda}\otimes {\bf I}_{k}\right)\right)\otimes{\bf I}_{n}\\

\end{array}
\end{equation}

and so ${\bm \Lambda}^{-1}=\left({\bf I}_{k}-{\bm \psi}\left({\bm \lambda}\otimes {\bf I}_{k}\right)\right)^{-1}\otimes{\bf I}_{n}$ and $|{\bm \Lambda}| = |{\bf I}_{k}-{\bm \psi}\left({\bm \lambda}\otimes {\bf I}_{k}\right)|^{n}$

Mean centering responses can help mixing, because ${\bm theta}$ and ${\bm lambda}$ are not sampled in a block. (Jarrod - I guess when $|{\bm \Lambda}|=1$ this could be detected and updating occur in a block?)
\section{Antedependence}

An $n\times n$ unstructured covariance matrix can be reparameterised in terms of regression coefficeints and residual variances from a set of $n$ nested multiple regressions. For example, for $n=3$ the following 3 multiple regressions can be defined:

\begin{equation}
\begin{array}{rl}
u_{3} =&  u_{2}\beta_{3|2}+u_{1}\beta_{3|1}+e_{u_{3}}\\ 
u_{2} =&  u_{1}\beta_{2|1}+e_{u_{2}}\\ 
u_{1} =&  e_{u_{1}}\\ 
\end{array}
\end{equation}

Arranging the regression coefficients and residual (`innovation') variances into a lower triangular matrix and diagonal matrix respectively:

\begin{equation}
{\bf L}_{u}=
\left[
\begin{array}{ccc}
1&0&0\\
-\beta_{2|1}&1&0\\
-\beta_{3|1}&-\beta_{3|2}&1\\
\end{array}
\right]
\end{equation} 

and 

\begin{equation}
{\bf D}_{u}=
\left[
\begin{array}{ccc}
\sigma^{2}_{e_{u_{1}}}&0&0\\
0&\sigma^{2}_{e_{u_{2}}}&0\\
0&0&\sigma^{2}_{e_{u_{3}}}\\
\end{array}
\right]
\end{equation} 


gives

\begin{equation}
{\bf V}_{u} =  {\bf L}_{u}{\bf D}_{u}{\bf L}_{u}^{\top}
\end{equation}

Rather than fit the saturated model (in this case all 3 regression coefficients) $k^{th}$ order antedependence models seek to model ${\bf V}_{u}$ whilst constraining the regression coefficients in ${\bf L}_{u}$ to be zero if they are on sub-diagonals>$k$. For example, a first order antedependence model would set the regression coefficients in the second off-diagonal (i.e. $\beta_{3|1}$) to zero, but estimate those in the first sub-diagonal (i.e. $\beta_{2|1}$ and $\beta_{3|2}$). For a 3$\times$3 matrix, a second order antedependence model would fit a fully unstructured covariance matrix. In terms of Gibbs sampling this parameterisation is less efficient because ${\bf V}_{u}$ is sampled in two blocks (the regression coefficients followed by the innovation variances) rather than in a single block from the inverse Wishart. However, more flexible conjugate prior specifications are possible by placing multivariate normal priors on the regression coefficients and independent inverse Wishart priors on the innovation variances. By constraining arbitrary regression coefficients to be zero in a fully unstructured model allows any fully recursive path model to be constructed for a set of random effects.

\section{Scaling}

The path analyses described above essentially allow elements of the response vector to be regressed on each other. Regressing an observation on itself would seem like a peculiar thing to do, although with a little work we can show that by doing this we can allow two sets of observations to conform to the same model except for a difference in scale. 



\ifalone
\end{document}
\else
\fi

